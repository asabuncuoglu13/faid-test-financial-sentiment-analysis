{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse FinBERT using LIT\n",
    "\n",
    "To analyse the FinBERT model using the Language Interpretability Tool (LIT), you would need to integrate the model into LIT's environment, allowing you to interactively explore its behavior, interpret predictions, and test for fairness, robustness, and other aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_nlp.api import model as lit_model\n",
    "from lit_nlp.api import dataset as lit_dataset\n",
    "from lit_nlp.api import types as lit_types\n",
    "from lit_nlp import notebook\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, pipeline\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to load the FinBERT model within a custom model class that LIT can recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asabuncuoglu/Documents/faid-test-financial-sentiment-analysis/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class FinBERTModel(lit_model.Model):\n",
    "    \"\"\"A wrapper for FinBERT to work with LIT.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load FinBERT model and tokenizer\n",
    "        self.model_name = \"yiyanghkust/finbert-tone\"\n",
    "        self.model = BertForSequenceClassification.from_pretrained(self.model_name, num_labels=3)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.config = BertConfig.from_pretrained(self.model_name)\n",
    "\n",
    "    def _load_model(self):\n",
    "        return BertForSequenceClassification.from_pretrained(self.model_name, num_labels=3)\n",
    "    \n",
    "    def _load_tokenizer(self):\n",
    "        return BertTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "    def input_spec(self) -> lit_types.Spec:\n",
    "        return {\n",
    "            \"text\": lit_types.TextSegment()\n",
    "        }\n",
    "    \n",
    "    def output_spec(self) -> lit_types.Spec:\n",
    "        return {\n",
    "            \"score\": lit_types.MulticlassPreds(vocab=[\"Positive\", \"Neutral\", \"Negative\"], parent=\"label\"),\n",
    "            \"label\": lit_types.CategoryLabel(vocab=[\"Positive\", \"Neutral\", \"Negative\"]),\n",
    "        }\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        # create a list of strings from the input\n",
    "        # input_list = []\n",
    "        results = []\n",
    "        for input in inputs:\n",
    "            # input_list.append(input[\"text\"])\n",
    "            \n",
    "            # if you don't want to use the pipeline, you can use the model directly\n",
    "            with torch.no_grad():\n",
    "                encoded_input = self.tokenizer(input[\"text\"], padding=True, return_tensors='pt')\n",
    "                output = self.model(**encoded_input)\n",
    "                probs = torch.softmax(output['logits'], dim=1)\n",
    "                label = self.config.id2label[torch.argmax(probs).item()]\n",
    "                results.append({\n",
    "                    \"score\": probs[0].tolist(),\n",
    "                    \"label\": label\n",
    "                })\n",
    "\n",
    "        # pipe = pipeline(\"text-classification\", model=self.model, tokenizer=self.tokenizer)\n",
    "        # results = pipe(input_list)\n",
    "        return results\n",
    "\n",
    "# Instantiate the FinBERT model\n",
    "model = FinBERTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinPhrasebankDataset(lit_dataset.Dataset):\n",
    "  \"\"\"Loader for MultiNLI development set.\"\"\"\n",
    "\n",
    "  TEXT_COLUMN = \"text\"\n",
    "  TARGET_COLUMN = \"sentiment\"\n",
    "\n",
    "  def __init__(self, path: str):\n",
    "    df = pd.read_csv(path, names=[self.TARGET_COLUMN, self.TEXT_COLUMN], encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "    # Store as a list of dicts, conforming to self.spec()\n",
    "    df = df.head(20) # limit to 20 examples for speed\n",
    "    self._examples = pd.DataFrame.to_dict(df, orient=\"records\")\n",
    "\n",
    "  def spec(self) -> lit_types.Spec:\n",
    "    return {\n",
    "      'text': lit_types.TextSegment(),\n",
    "      'sentiment': lit_types.CategoryLabel(vocab=[self.TARGET_COLUMN, self.TEXT_COLUMN])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After wrapping the FinBERT model, you need to launch the LIT server to interactively analyze the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/external/financialphrasebank.csv\"\n",
    "\n",
    "# Initialize dataset and model\n",
    "dataset = FinPhrasebankDataset(path=filename)\n",
    "model = FinBERTModel()\n",
    "\n",
    "# Create the LIT widget\n",
    "lit_widget = notebook.LitWidget(models={\"finbert\": model}, datasets={\"finphrasebank\": dataset}, port=8890)\n",
    "#lit_widget.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the server is running, you can access LIT in your web browser by navigating to `http://localhost:8890`. Or optionally, you can run it on your notebook:\n",
    "\n",
    "```python\n",
    "widget.render(height=600)\n",
    "```\n",
    "\n",
    "Note that, I could not run this widget on VSCode's Jupyter Notebook Editor, so I only tested the functionality on the browser. In the LIT interface, you can interactively explore the model's predictions, compare different inputs, visualize feature importance, and analyze model behavior through various interpretability techniques.\n",
    "\n",
    "### Analyze Model Behavior\n",
    "- **Examine Predictions**: Input different financial texts and observe how FinBERT classifies the sentiment. You can compare results across multiple texts or even multiple models if loaded.\n",
    "- **Saliency Maps**: Use saliency maps to understand which words contribute most to the sentiment classification.\n",
    "- **Counterfactuals**: Test counterfactual scenarios by modifying parts of the text to see how the model’s prediction changes, helping you understand its decision-making process.\n",
    "- **Fairness Analysis**: Analyze how the model performs across different types of financial data (e.g., news articles about different industries or regions) to check for any biases.\n",
    "\n",
    "This setup enables you to ensure the model’s fairness, understand its decision-making process, and refine its performance for financial sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
