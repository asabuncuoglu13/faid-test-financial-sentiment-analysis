# Bias Identification and Mitigation in Financial Sentiment Analysis

Hovy et al. summarises the potential biases in NLP systems in five categories [^1]:

*   **Bias from Data**: NLP systems can inherit biases present in the training data. For instance, many datasets are derived from news sources, which are often created by a homogeneous group of individuals (white, middle-aged, educated, upper-middle-class men). This can result in models that perform poorly for demographics not well-represented in the training data, like those under 35 or ethnic minorities.  This is known as **selection bias**.
*   **Bias from Annotations**: The process of annotating data can introduce **label bias** due to (1) annotator error (distraction, lack of interest, etc.), (2) systematic disagreements among well-meaning annotators, (3) mismatches between annotators' and authors' social and linguistic norms can result in biased interpretations, (4) crowdsourcing, while cost-effective, can exacerbate these issues due to the difficulty in training and managing a large number of annotators.
*   **Bias from Input Representations**: Even when data is balanced and well-annotated, biases can seep in through input representations like word embeddings.  These embeddings, trained on large text corpora, have been found to capture and perpetuate societal biases, such as associating "woman" with "homemaker" and "man" with "programmer".  This is referred to as **semantic bias**. This also extends to contextualized representations from pre-trained language models, which are trained on massive amounts of internet data, further amplifying societal biases.
*   **Bias from Models**: The models themselves can amplify existing biases present in the data, leading to **bias overamplification**. Models exploiting spurious correlations or statistical irregularities in the data to achieve higher accuracy, even if those correlations reflect societal biases. For example, if all positive examples in the training data come from female authors, the model might incorrectly use gender as a predictive feature. Models making predictions even when uncertain or lacking information, potentially resulting in incorrect outputs that reinforce biases. For instance, a machine translation tool might translate a gender-neutral phrase into a gendered one based on societal stereotypes, even if that was not the intended meaning.
*   **Bias from Research Design**: The overall research design can contribute to bias, often stemming from an overexposure to certain languages or topics and the underrepresentation of others. The historical dominance of English in NLP research has led to an **overexposure bias**, where researchers are more likely to work on well-resourced languages and tasks. This perpetuates the cycle, making it more challenging to work on other languages due to the lack of resources. While efforts towards multilingual and cross-lingual approaches are increasing, English remains a dominant force due to commercial incentives. The lack of demographic diversity within research groups can further contribute to bias as marginalized communities may not have their perspectives proportionally represented.

In this repo, we will explore the evaluation and mitigation of these potential biases, focusing on financial sentiment analysis use-case.

**Research Preview Disclaimer:** This repository contains code, data, and materials that are part of an ongoing research project. Please note that the work is in a **research preview** stage and may produce incomplete or inconsistent results. Use at your own discretion, and feel free to contribute or report any issues.

[^1]: Hovy, D., & Prabhumoye, S. (2021). Five sources of bias in natural language processing. Language and Linguistics Compass, e12432. <https://doi.org/10.1111/lnc3.12432>