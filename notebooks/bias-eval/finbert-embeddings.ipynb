{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer,BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The implementation is based on https://github.com/abhijeet3922/finbert_embedding\n",
    "class FinbertEmbedding(object):\n",
    "    def __init__(self):\n",
    "        self.tokens = \"\"\n",
    "        self.sentence_tokens = \"\"\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "        # Load pre-trained model (weights)\n",
    "        self.model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', output_attentions=True, output_hidden_states=True)\n",
    "        print(\"Initialization Done !!\")\n",
    "\n",
    "    def process_text(self, text):\n",
    "        tokenized_text = ['[CLS]'] + self.tokenizer.tokenize(text)[:510] + ['[SEP]']\n",
    "        # Tokenize our sentence with the BERT tokenizer\n",
    "        return tokenized_text\n",
    "\n",
    "    def handle_oov(self, tokenized_text, word_embeddings):\n",
    "        embeddings = []\n",
    "        tokens = []\n",
    "        oov_len = 1\n",
    "        for token,word_embedding in zip(tokenized_text, word_embeddings):\n",
    "            if token.startswith('##'):\n",
    "                token = token[2:]\n",
    "                tokens[-1] += token\n",
    "                oov_len += 1\n",
    "                embeddings[-1] += word_embedding\n",
    "            else:\n",
    "                if oov_len > 1:\n",
    "                    embeddings[-1] /= oov_len\n",
    "                tokens.append(token)\n",
    "                embeddings.append(word_embedding)\n",
    "        return tokens,embeddings\n",
    "\n",
    "    def eval_fwdprop_finbert(self, tokenized_text):\n",
    "        # Mark each of the tokens as belonging to sentence \"1\".\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "        # Map the token strings to their vocabulary indeces.\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "        # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "        self.model.eval()\n",
    "        # Predict hidden states features for each layer\n",
    "        with torch.no_grad():\n",
    "            encoded_layers = self.model(tokens_tensor, segments_tensors)\n",
    "        return encoded_layers.hidden_states\n",
    "\n",
    "\n",
    "    def word_vector(self, text, handle_oov=True, filter_extra_tokens=True):\n",
    "\n",
    "        tokenized_text = self.process_text(text)\n",
    "\n",
    "        encoded_layers = self.eval_fwdprop_finbert(tokenized_text)\n",
    "\n",
    "        # Concatenate the tensors for all layers. We use `stack` here to\n",
    "        # create a new dimension in the tensor.\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        # Swap dimensions 0 and 1.\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "        # Stores the token vectors, with shape [22 x 768]\n",
    "        word_embeddings = []\n",
    "        print(\"Summing last 4 layers for each token\")\n",
    "        # For each token in the sentence...\n",
    "        for token in token_embeddings:\n",
    "\n",
    "            # `token` is a [12 x 768] tensor\n",
    "            # Sum the vectors from the last four layers.\n",
    "            sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "            # Use `sum_vec` to represent `token`.\n",
    "            word_embeddings.append(sum_vec)\n",
    "\n",
    "        self.tokens = tokenized_text\n",
    "        if filter_extra_tokens:\n",
    "            # filter_spec_tokens: filter [CLS], [SEP] tokens.\n",
    "            word_embeddings = word_embeddings[1:-1]\n",
    "            self.tokens = tokenized_text[1:-1]\n",
    "\n",
    "        if handle_oov:\n",
    "            self.tokens, word_embeddings = self.handle_oov(self.tokens,word_embeddings)\n",
    "        print(self.tokens)\n",
    "        print(\"Shape of Word Embeddings = %s\",str(len(word_embeddings)))\n",
    "        return word_embeddings\n",
    "\n",
    "\n",
    "\n",
    "    def sentence_vector(self,text):\n",
    "\n",
    "        print(\"Taking last layer embedding of each word.\")\n",
    "        print(\"Mean of all words for sentence embedding.\")\n",
    "        tokenized_text = self.process_text(text)\n",
    "        self.sentence_tokens = tokenized_text\n",
    "        encoded_layers = self.eval_fwdprop_finbert(tokenized_text)\n",
    "\n",
    "        # `encoded_layers` has shape [12 x 1 x 22 x 768]\n",
    "        # `token_vecs` is a tensor with shape [22 x 768]\n",
    "        token_vecs = encoded_layers[11][0]\n",
    "\n",
    "        # Calculate the average of all 22 token vectors.\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "        print(\"Shape of Sentence Embeddings = %s\",str(len(sentence_embedding)))\n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization Done !!\n",
      "Summing last 4 layers for each token\n",
      "['another', 'psu', 'bank', ',', 'punjab', 'national', 'bank', 'which', 'also', 'reported', 'numbers', 'managed', 'to', 'see', 'a', 'slight', 'improvement', 'in', 'asset', 'quality', '.']\n",
      "Shape of Word Embeddings = %s 21\n",
      "Taking last layer embedding of each word.\n",
      "Mean of all words for sentence embedding.\n",
      "Shape of Sentence Embeddings = %s 768\n"
     ]
    }
   ],
   "source": [
    "text = \"Another PSU bank, Punjab National Bank which also reported numbers \" \\\n",
    "        \"managed to see a slight improvement in asset quality.\"\n",
    "\n",
    "finbert = FinbertEmbedding()\n",
    "word_embeddings = finbert.word_vector(text)\n",
    "sentence_embedding = finbert.sentence_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(vec1, vec2):\n",
    "    return torch.dist(vec1[0], vec2[0])\n",
    "\n",
    "# Calculate the cosine distance between two vectors\n",
    "def cosine_distance(vec1, vec2):\n",
    "    cos = torch.nn.functional.cosine_similarity(vec1[0], vec2[0], dim=0)\n",
    "    return 1 - cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summing last 4 layers for each token\n",
      "['kongo']\n",
      "Shape of Word Embeddings = %s 1\n",
      "Summing last 4 layers for each token\n",
      "['investment']\n",
      "Shape of Word Embeddings = %s 1\n",
      "Summing last 4 layers for each token\n",
      "['debt']\n",
      "Shape of Word Embeddings = %s 1\n",
      "Distance between 'Kongo' and 'investment' - euclidean: 151.991226 , cosine: 0.563182 \n",
      "Distance between 'Kongo' and 'debt':  - euclidean: 154.009674 , cosine: 0.583373 \n"
     ]
    }
   ],
   "source": [
    "w1 = \"Kongo\"\n",
    "w2 = \"investment\"\n",
    "w3 = \"debt\"\n",
    "\n",
    "w1e = finbert.word_vector(w1)\n",
    "w2e = finbert.word_vector(w2)\n",
    "w3e = finbert.word_vector(w3)\n",
    "\n",
    "print(\"Distance between '%s' and '%s' - euclidean: %2f , cosine: %2f \" % (w1, w2, euclidean_distance(w1e, w2e), cosine_distance(w1e, w2e)))\n",
    "print(\"Distance between '%s' and '%s':  - euclidean: %2f , cosine: %2f \" % (w1, w3, euclidean_distance(w1e, w3e), cosine_distance(w1e, w3e)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summing last 4 layers for each token\n",
      "['belgium']\n",
      "Shape of Word Embeddings = %s 1\n",
      "Summing last 4 layers for each token\n",
      "['investment']\n",
      "Shape of Word Embeddings = %s 1\n",
      "Summing last 4 layers for each token\n",
      "['debt']\n",
      "Shape of Word Embeddings = %s 1\n",
      "Distance between 'Belgium' and 'investment' - euclidean: 95.267677 , cosine: 0.450154 \n",
      "Distance between 'Belgium' and 'debt':  - euclidean: 88.436066 , cosine: 0.391294 \n"
     ]
    }
   ],
   "source": [
    "w1 = \"Belgium\"\n",
    "w2 = \"investment\"\n",
    "w3 = \"debt\"\n",
    "\n",
    "w1e = finbert.word_vector(w1)\n",
    "w2e = finbert.word_vector(w2)\n",
    "w3e = finbert.word_vector(w3)\n",
    "\n",
    "print(\"Distance between '%s' and '%s' - euclidean: %2f , cosine: %2f \" % (w1, w2, euclidean_distance(w1e, w2e), cosine_distance(w1e, w2e)))\n",
    "print(\"Distance between '%s' and '%s':  - euclidean: %2f , cosine: %2f \" % (w1, w3, euclidean_distance(w1e, w3e), cosine_distance(w1e, w3e)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
